#!/usr/bin/env bash


#########################################
#infor aboutt he cluster 
#cluster= smp (default)
#partition= smp (default)
#100 nodes of 24-core Xeon Gold 6126 2.60 GHz (Skylake)
#192 GB RAM
#256 GB SSD & 500 GB SSD
#10GigE

##cluster= mpi
##partition= opa (default)
##96 nodes of 28-core Intel Xeon E5-2690 2.60 GHz (Broadwell)
##64 GB RAM/node
##256 GB SSD
##100 Gb Omni-Path
##partition= ib
##32 nodes of 20-core Intel Xeon E5-2660 2.60 GHz (Haswell)
##128 GB RAM/node
##56 Gb FDR


#########################################

#SBATCH --time=0-05:30:00


##number of nodes should be bigger than one otherwise it will complain
##SBATCH --cluster=mpi
##SBATCH --partition=ib
##SBATCH --partition=opa

#########################################

#    Some Namings and email 

#########################################

#SBATCH --job-name=shb105
#SBATCH --output=Max.out
#SBATCH --mail-user=shb105@pitt.edu
#SBATCH --mail-type=END,FAIL 

########################################## 

#           Node Count 

########################################## 


#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4	

## if you specify only this the system automatically finds out the number of nodes
## SBATCH --ntasks=9

# -O is used to overspecify the cores, especially beneficial for debugging
## SBATCH -O
###########################################

             # Load modules

###########################################
# module purge removes all the previously loaded modules
#module purge

module load intel/2017.1.132
module load intel-mpi/2017.1.132
module load hdf5/1.10.0
module load  cmake/3.7.1
module load parmetis/4.0.3
module load zoltan/3.83.0


mpirun -np 4  ./bin/amrGem input/bunny.stl 2 2

##mpirun -np $SLURM_NTASKS  ./bin/amrGem input/bunny.stl 6 6






