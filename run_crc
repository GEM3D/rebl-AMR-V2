#!/usr/bin/env bash


#########################################
#infor aboutt he cluster 

#cluster= mpi

 #   partition= opa (default)
 #      96 nodes of 28-core Intel Xeon E5-2690 2.60 GHz (Broadwell)
 #      64 GB RAM/node
 #       256 GB SSD
 #       100 Gb Omni-Path
 #   partition= ib
 #       32 nodes of 20-core Intel Xeon E5-2660 2.60 GHz (Haswell)
 #       128 GB RAM/node
 #       56 Gb FDR


#########################################

#SBATCH --time=0-05:30:00

##SBATCH --cluster=smp
##SBATCH --partition=smp

## number of nodes should be bigger than one otherwise it will complain
#SBATCH --cluster=mpi
##SBATCH --partition=ib
#SBATCH --partition=opa

#########################################

#    Some Namings and email 

#########################################

#SBATCH --job-name=shb105
#SBATCH --output=Max.out
#SBATCH --mail-user=shb105@pitt.edu
#SBATCH --mail-type=END,FAIL 

########################################## 

#           Node Count 

########################################## 


#SBATCH --nodes=2
#SBATCH --ntasks-per-node=28

## if you specify only this the system automatically finds out the number of nodes
##SBATCH --ntasks=9

# -O is used to overspecify the cores, especially beneficial for debugging
##SBATCH -O
###########################################

             # Load modules

###########################################
# module purge removes all the previously loaded modules
#module purge

module load intel/2017.1.132
module load intel-mpi/2017.1.132
module load hdf5/1.10.0
module load  cmake/3.7.1
module load parmetis/4.0.3
module load zoltan/3.83.0


mpirun -np $SLURM_NTASKS ./bin/amrGem input/Perdigao_12k_5-7k_0-8k.stl 8 6

##mpirun -np $SLURM_NTASKS  ./bin/amrGem input/nasaplane.stl 6 6






